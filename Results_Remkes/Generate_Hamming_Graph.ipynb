{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the initialization and imports\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "from string import ascii_lowercase\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from Bio import SeqIO, AlignIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Emboss.Applications import NeedleallCommandline\n",
    "\n",
    "# Demand Python 3.\n",
    "if sys.version_info[0] < 3:\n",
    "    print(\"Python 3 is required, but you are using Python %i.%i.%i\") % (\n",
    "        sys.version_info[0], sys.version_info[1], sys.version_info[2])\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the specific functions from ind and proteins.py\n",
    "indels_path=\"/home/maya/InDelScanner\"  # /PATH/TO/InDelScanner\n",
    "if indels_path not in sys.path:\n",
    "    sys.path.append(indels_path)\n",
    "\n",
    "from ipynb.fs.defs.Library_diversity import convert_variant_to_dict, single_fraction_enrichment\n",
    "\n",
    "os.chdir(\"/mnt/c/Users/Maya/Documents/03_Kinases/\")\n",
    "\n",
    "with open('mek.pickle', 'rb') as pf:\n",
    "    mek = pickle.load(pf)\n",
    "\n",
    "with open('df_pos.pickle', 'rb') as pf:\n",
    "    df_pos = pickle.load(pf)\n",
    "    \n",
    "pos = df_pos.to_dict()\n",
    "\n",
    "# Set general restrictions stemming from SpliMLib library design\n",
    "aa_2 = ['A', 'Δ']\n",
    "aa_12 = ['A','G','P','Y','D','K','M','V','I','L','F','W']\n",
    "aa_13 = aa_12 +  ['Δ']\n",
    "splimlib = {'6': aa_12, '9': aa_12, '11': aa_12, '13': aa_12, '7a': aa_13, '8a': aa_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ls = df_pos.index.tolist()\n",
    "active = {short : convert_variant_to_dict(short) for short in active_ls}\n",
    "valid_pos = ['6', '7a', '8a', '9', '11', '13']\n",
    "\n",
    "data = {}\n",
    "for short, m_to_pos in active.items():\n",
    "    if len(m_to_pos) != len(valid_pos):\n",
    "        continue\n",
    "    else:\n",
    "        data[short] = [m_to_pos[i] for i in valid_pos]\n",
    "\n",
    "factors = pd.DataFrame.from_dict(data, orient='index', columns=valid_pos).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors.to_pickle('mkk_factors.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = random.sample(active.items(), 1000)\n",
    "test_dict = {}\n",
    "for i in range(len(test)):\n",
    "    test_dict[test[i][0]] = test[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamming distance = 0 if the aa matches at all positions\n",
    "def hamming_distance(s1, s2, splimlib):\n",
    "    d = 0\n",
    "    if (len(s1) != len(splimlib)) or (len(s2) != len(splimlib)):\n",
    "        d += 2\n",
    "\n",
    "    for p in splimlib.keys():\n",
    "        if s1[p] != s2[p]:\n",
    "            d += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of edges\n",
    "def list_edge_nodes(active, splimlib):\n",
    "    node_pairs = []\n",
    "    for short1, s1 in active.items():\n",
    "        for short2, s2 in active.items():\n",
    "            h_dis = hamming_distance(s1, s2, splimlib)\n",
    "            if h_dis == 1:\n",
    "                node_pairs.append((short1, short2))\n",
    "    return node_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = '<?xml version=\"1.0\" encoding=\"utf-8\"?>'\n",
    "graphml_open = '<graphml xmlns=\"http://graphml.graphdrawing.org/xmlns\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \\\n",
    "    xsi:schemaLocation=\"http://graphml.graphdrawing.org/xmlns  http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\">'\n",
    "graph_open = '<graph edgedefault=\"undirected\">'\n",
    "\n",
    "close = '</graph>\\n</graphml>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPHML functions\n",
    "\n",
    "# create nodes\n",
    "def gml_write_node(short, file):\n",
    "    print('<node id=\\\"{node}\"/>'.format(node=short), sep=\"\", file=file)\n",
    "\n",
    "def gml_list_edge_strings(node_pairs):\n",
    "    edge_strings = []\n",
    "    for pair in node_pairs:\n",
    "        edge_id = '-'.join(pair)\n",
    "        one_edge = '<edge id=\"{id}\" source=\"{s}\" target=\"{t}\"/>'.format(\n",
    "            id=edge_id, s=pair[0], t=pair[1])\n",
    "        edge_strings.append(one_edge)\n",
    "    return edge_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write the graph file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_graphml(graph_dict, filename):\n",
    "    with open (filename, 'w') as f:\n",
    "        # print the GRAPHML header definitions\n",
    "        print(header, graphml_open, graph_open, file=f, sep='\\n')\n",
    "\n",
    "        # write nodes\n",
    "        for short in graph_dict.keys():\n",
    "            gml_write_node(short, f)\n",
    "\n",
    "        # write edges\n",
    "        node_pairs = list_edge_nodes(graph_dict, splimlib)\n",
    "        edge_strings = gml_list_edge_strings(node_pairs)\n",
    "        f.writelines(edge_strings)\n",
    "\n",
    "        # end the graph and graphml\n",
    "        print(close, file=f)\n",
    "    \n",
    "    return node_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_edges = write_graphml(active, 'active.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence similarity network analysis with NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ham = nx.read_graphml('active.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVERAGE NODE DEGREE\n",
    "g_ham.size() / g_ham.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_components = list(g_ham.subgraph(c) for c in nx.connected_components(g_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ham_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is justified to only examine the largest connected subgraph, since the next subgraphs have very few nodes: 5 (2x), 4 (4x), 3 (12x) or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_lens = {i: len(ham_components[i]) for i in range(len(ham_components))} # get the number of nodes in the largest subgraphs\n",
    "\n",
    "sort_components = sorted(component_lens.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_components[:10]:\n",
    "\tprint(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = ham_components[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.size()/G.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = sorted([d for n, d in G.degree()], reverse=True)  # degree sequence\n",
    "# print \"Degree sequence\", degree_sequence\n",
    "degreeCount = Counter(degree_sequence)\n",
    "deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "plt.title(\"Largest connected subgraph\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = sorted([d for n, d in g_ham.degree()], reverse=True)  # degree sequence\n",
    "# print \"Degree sequence\", degree_sequence\n",
    "degreeCount = Counter(degree_sequence)\n",
    "deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "plt.title(\"All nodes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking promising. At this point, take the largest subgraph into Gephi to build a good visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, 'Largest_connected_26563vars.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.degree('6P/9I/11L/13P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.Graph()\n",
    "# copy over the nodes from G\n",
    "for n in G.nodes():\n",
    "    H.add_node(n, deg=G.degree[n])\n",
    "for e in G.edges():\n",
    "    H.add_edge(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = nx.clustering(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the relationship between node degree and node clustering coefficient?\n",
    "node_cluster = {}\n",
    "for n in H.nodes:\n",
    "    d = H.degree(n) # get node degree\n",
    "    c = cl[n] # get cluster coeffiecient\n",
    "    try:\n",
    "        node_cluster[d].append(c)\n",
    "    except KeyError:\n",
    "        node_cluster[d] = [c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the clustering coefficients\n",
    "nc_avg = OrderedDict()\n",
    "for deg in sorted(node_cluster):\n",
    "    nc_avg[deg] = sum(node_cluster[deg])/len(node_cluster[deg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# build a box plot\n",
    "ax.boxplot(list(node_cluster.values()))\n",
    "ax.plot(deg[1:], cnt[1:], color='b')\n",
    "ax.set_xlabel(\"Node degree\")\n",
    "ax.set_ylabel(\"Distribution of the clustering coeffiecient\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg, cnt = zip(*nc_avg.items())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(deg[1:], cnt[1:], color='b')\n",
    "plt.ylim([0, 0.3])\n",
    "plt.ylabel(\"Average clustering coefficient\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start experimenting with pruning.\n",
    "\n",
    "https://stackoverflow.com/questions/18381187/functions-for-pruning-a-networkx-graph\n",
    "\n",
    "One option is to try pruning nodes according to betweeness centrality. First, calculate the betweeness centrality for all nodes, identifying the ones that are 'hubs': appear in many shortest paths between nodes. Plot a distribution on betcen to get an idea of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closeness centrality\n",
    "clo_cen = nx.closeness_centrality(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H_closeness_centrality.pickle', 'wb') as f:\n",
    "    pickle.dump(clo_cen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness centrality\n",
    "bet_cen = nx.betweenness_centrality(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H_betweenness_centrality.pickle', 'wb') as f:\n",
    "    pickle.dump(clo_cen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_cen = nx.eigenvector_centrality(H)\n",
    "\n",
    "with open('H_eigenvector_centrality.pickle', 'wb') as f:\n",
    "    pickle.dump(eig_cen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(H, clo_cen, 'closeness')\n",
    "nx.set_node_attributes(H, bet_cen, 'betweenness')\n",
    "nx.set_node_attributes(H, eig_cen, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(H.degree(H.nodes()))\n",
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)\n",
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_short = '6P/9I/11L/13P'\n",
    "wt = {'6': 'P', '7a': 'Δ', '8a': 'Δ', '9': 'I', '11': 'L', '13': 'P'}\n",
    "\n",
    "H_dist = {}\n",
    "for n in H.nodes():\n",
    "    s1 = convert_variant_to_dict(n)\n",
    "    H_dist[n] = hamming_distance(wt, s1, splimlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(H, H_dist, \"Hamming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(H, 'connected_annotated.graphml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
